{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DQN Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO is motivated by the following question: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? The main idea is that after an update, the new policy should be not too far form the old policy. For that, PPO (as implemented in Stable Baselines) uses clipping to avoid too large update.\n",
    "\n",
    "\n",
    "### Quick Facts\n",
    "\n",
    "* DQN is an off-policy algorithm.\n",
    "* DQN can only be used for environments with discrete action spaces.\n",
    "* The Stable Baselines implementation of PPO follows the OpenAI made for GPU. For multiprocessing, it uses vectorized environment.\n",
    "\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted, cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where $R_{t_0}$ is also known as the *return*. The discount, $\\gamma$, should be a constant between $0$ and $1$ that ensures the sum converges. It makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function $Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{equation}\n",
    " \\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have access to $Q^*$. But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$ function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{equation}\n",
    " Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\n",
    "\\end{equation}\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{equation}\n",
    " \\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\n",
    "\\end{equation}\n",
    "\n",
    "To minimise this error, we will use the `Huber loss` (https://en.wikipedia.org/wiki/Huber_loss). The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of $Q$ are very noisy. We calculate this over a batch of transitions, $B$, sampled from the replay memory:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "\\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    " \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    " |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "### Replay Buffer\n",
    "We’ll be using experience replay memory for training our DQN. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure\n",
    "\n",
    "\n",
    "### Exploration vs. Exploitation\n",
    "\n",
    "Stable Baselines DQN implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning, the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DQN exploration.\n",
    "\n",
    "\n",
    "### Implementation Notes (Stable Baselines)\n",
    "\n",
    "By default, the DQN class has double q learning and dueling extensions enabled. \n",
    "\n",
    "#### Can I use?\n",
    "\n",
    "-  Recurrent policies: ❌\n",
    "-  Multi processing: ❌\n",
    "-  Gym spaces:\n",
    "\n",
    "\n",
    "\n",
    "| Space         | Action | Observation |  \n",
    "| --- | --- | --- |   \n",
    "|Discrete |      ✔️|      ✔️ |  \n",
    "|Box      |      ❌ |      ✔️ | \n",
    "|MultiDiscrete | ❌ |      ✔️ | \n",
    "|MultiBinary |   ❌ |      ✔️ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import DQN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training Configuration\n",
    "### Parameters\n",
    "\n",
    "* **ENV_NAME**: the name of the Gym Environment for training\n",
    "* **EVAL_STEPS**: number of samples to be used for policy evaluation. Here, the horizon $H$ is 200 steps, so the evaulation is 2000 / 200 = 10 episodes\n",
    "* **ITERS**: number of simulation time-steps during training. Here corresponds to 250 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "EVAL_STEPS = 2000\n",
    "ITERS = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Logging\n",
    "### Logging Directories\n",
    "\n",
    "* **PPO2_Cartpole**: the directory used for the tensorboard\n",
    "* **Results**: the directory where the intermediate trained agents are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logs_gen(net_size):\n",
    "    tensorboard_log = \"./DQN_Cartpole/2x\" + str(net_size)\n",
    "    log_dir = \"./Results/\"\n",
    "    return log_dir, tensorboard_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Environment Definition\n",
    "### Vectorized Environments\n",
    "\n",
    "Vectorized Environments are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. Because of this, actions passed to the environment are now a vector (of dimension n). It is the same for observations, rewards and end of episode signals (dones). In the case of non-array observation spaces such as Dict or Tuple, where different sub-spaces may have different shapes, the sub-observations are vectors (of dimension n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Agent Definition\n",
    "### Agent Parameters\n",
    "\n",
    "* **MlpPolicy**: The policy model to use (MlpPolicy, CnnPolicy, CnnLstmPolicy, …)\n",
    "* **policy_kwargs**: the Network Architecture (here 2 hiden layers with size 256)\n",
    "* **env**: the Gym Environment\n",
    "* **learning_rate**: learning rate for adam optimizer\n",
    "* **buffer_size**: size of the replay buffer\n",
    "* **gamma**: Discount factor\n",
    "* **exploration_fraction**:  fraction of entire training period over which the exploration rate is annealed\n",
    "* **exploration_final_eps**: final value of random action probability\n",
    "* **exploration_initial_eps**: initial value of random action probability\n",
    "* **train_freq**: update the model every train_freq steps\n",
    "* **batch_size**: size of a batched sampled from replay buffer for training\n",
    "* **double_q**: Whether to enable Double-Q learning or not (always do this!)\n",
    "* **learning_starts**: how many steps of the model to collect transitions for before learning starts\n",
    "* **target_network_update_freq**: update the target network every target_network_update_freq steps\n",
    "* **prioritized_replay**: if True prioritized replay buffer will be used\n",
    "* **tensorboard_log**: the log location for tensorboar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/policies.py:109: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 256\n",
    "policy_kwargs = dict(layers=[hidden_layer_size, hidden_layer_size])\n",
    "log_dir, tensorboard_log = logs_gen(hidden_layer_size)\n",
    "\n",
    "model = DQN(MlpPolicy, env, gamma=0.99, learning_rate=0.001, buffer_size=50000, policy_kwargs=policy_kwargs,\n",
    "            exploration_fraction=0.1, exploration_final_eps=0.02, exploration_initial_eps=1.0,\n",
    "            train_freq=1, batch_size=32, double_q=True,\n",
    "            learning_starts=1000, target_network_update_freq=500,\n",
    "            prioritized_replay=False, verbose=1, tensorboard_log=tensorboard_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Define a callback function\n",
    "We can define a custom callback function that will be called inside the agent. This could be useful when we want to monitor training, for instance display live learning curves in Tensorboard or save the best agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "evaluation_rewards = []\n",
    "\n",
    "def callback(_locals, _globals):\n",
    "    \"\"\"\n",
    "  Callback called at each step\n",
    "  :param _locals: (dict)\n",
    "  :param _globals: (dict)\n",
    "  \"\"\"\n",
    "    global n_steps, best_mean_reward\n",
    "    # Print stats every 1000 calls\n",
    "    if (n_steps + 1) % 1000 == 0:\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        print(\"Evaluating Model: \" + str(n_steps))\n",
    "        rew = evaluate(model, num_steps=EVAL_STEPS, render=False)\n",
    "        print(\"Best mean reward: {:.15f} - Last mean reward per episode: {:.15f}\".format(best_mean_reward, rew))\n",
    "        _locals['self'].save(log_dir + 'model_' + str(n_steps))\n",
    "        if (rew > best_mean_reward):\n",
    "            best_mean_reward = rew\n",
    "            print(\"Saving new best model\")\n",
    "            _locals['self'].save(log_dir + 'best_model')\n",
    "    n_steps += 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Define an evaluation function\n",
    "\n",
    "During training the action selection is subject to the exploration scheme defined. This implies that policy A could be better than policy B but appear to perform worse due to the effect of the exploration.\n",
    "\n",
    "Due to this, in order to determine the best policy throughout the entire training phase, in each Callback we evaluate the current policy without exploration (deterministic actions); if the current policy is better than the best policy found so far, we simply override the best policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_steps=300, render=False):\n",
    "    global evaluation_rewards\n",
    "    print(\"EVALUATION!!!\")\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_steps: (int) number of timesteps to evaluate it\n",
    "    :return: (float) Mean reward\n",
    "    \"\"\"\n",
    "    episode_rewards = [[0.0] for _ in range(env.num_envs)]\n",
    "    obs = env.reset()\n",
    "    # print(obs)\n",
    "    for i in range(num_steps - 1):\n",
    "        # _states are only useful when using LSTM policies\n",
    "        actions, _ = model.predict(obs, deterministic=True)\n",
    "        # here, action, rewards and dones are arrays\n",
    "        # because we are using vectorized env\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "\n",
    "        if (render):\n",
    "            env.render()\n",
    "\n",
    "        for j in range(env.num_envs):\n",
    "            episode_rewards[j][-1] += rewards[j]\n",
    "            if dones[j]:\n",
    "                episode_rewards[j].append(0.0)\n",
    "\n",
    "    mean_rewards = [0.0 for _ in range(env.num_envs)]\n",
    "    n_episodes = 0\n",
    "    for i in range(env.num_envs):\n",
    "        mean_rewards[i] = np.mean(episode_rewards[i])\n",
    "        n_episodes += len(episode_rewards[i])\n",
    "\n",
    "    # Compute mean reward\n",
    "    mean_reward = round(np.mean(mean_rewards), 1)\n",
    "    print(\"Mean reward:\", mean_reward, \"Num episodes:\", n_episodes)\n",
    "    evaluation_rewards.append(mean_reward)\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Evaluate the Agent before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION!!!\n",
      "Mean reward: 29.0 Num episodes: 69\n"
     ]
    }
   ],
   "source": [
    "rew = evaluate(model, num_steps=EVAL_STEPS, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Train the DQN Agent \n",
    "![DQN Algorithm](img/DQN.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:322: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 28.2 Num episodes: 71\n",
      "Best mean reward: -inf - Last mean reward per episode: 28.199999999999999\n",
      "Saving new best model\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/dqn.py:283: The name tf.RunOptions is deprecated. Please use tf.compat.v1.RunOptions instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/mut/workspace/anaconda3/envs/py36/lib/python3.6/site-packages/stable_baselines/deepq/dqn.py:284: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.\n",
      "\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 28.199999999999999 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 58       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 21.6     |\n",
      "| steps                   | 2139     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 28.199999999999999 - Last mean reward per episode: 105.200000000000003\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 105.200000000000003 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 105.200000000000003 - Last mean reward per episode: 133.300000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 87.6     |\n",
      "| steps                   | 10899    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 20.8 Num episodes: 96\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 20.800000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 60.6 Num episodes: 33\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 60.600000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 60.6 Num episodes: 33\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 60.600000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 113      |\n",
      "| steps                   | 22170    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 58.8 Num episodes: 34\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 58.799999999999997\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 74.0 Num episodes: 27\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 74.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 35.1 Num episodes: 57\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 35.100000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 90.8     |\n",
      "| steps                   | 31253    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 83.3 Num episodes: 24\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 83.299999999999997\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 74.0 Num episodes: 27\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 74.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 37.7 Num episodes: 53\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 37.700000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 18.3 Num episodes: 109\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 18.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 92.8     |\n",
      "| steps                   | 40531    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 16.9 Num episodes: 118\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 16.899999999999999\n",
      "-----------------------------------------------------\n",
      "Training Time: 215.4886450767517\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.learn(total_timesteps=ITERS, callback=callback)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Training Time: \" + str(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Plot the total reward of the policies evaluated in the Callback (deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(evaluation_rewards)\n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Evaluation Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Evaluate the best Agent after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load(log_dir + 'best_model', env=env)\n",
    "r = evaluate(model, num_steps=EVAL_STEPS, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 Close environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.13 How each parameter affects the performance of the Algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hid_layer_height_list = [2, 64, 128, 256]\n",
    "hid_layer_list = [1,2]\n",
    "\n",
    "lr_list = [0.00001, 0.001, 0.1]\n",
    "buf_list = [32, 5000, 50000]\n",
    "solved = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Layer Size = 1x2\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: -inf - Last mean reward per episode: 9.300000000000001\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 19.9     |\n",
      "| steps                   | 1966     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 13.7     |\n",
      "| steps                   | 3333     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 13       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 11.1     |\n",
      "| steps                   | 4438     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 5395     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.9 Num episodes: 202\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.900000000000000\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 10.6     |\n",
      "| steps                   | 6454     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 10.1 Num episodes: 198\n",
      "Best mean reward: 9.900000000000000 - Last mean reward per episode: 10.100000000000000\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 7413     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 9.7      |\n",
      "| steps                   | 8378     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 9331     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 10280    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 11233    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 12162    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 13111    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 14051    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 14995    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 15944    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 16900    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 17852    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 9.7      |\n",
      "| steps                   | 18817    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 19778    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 20735    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 21685    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 22636    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 23564    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 24513    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 25459    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 26396    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 27334    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 28274    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 29210    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 30140    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 31076    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 32017    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 32969    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 33912    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 34864    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 35805    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 36742    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 37682    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 38618    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 39564    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 40506    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4200     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 41441    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4300     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 42375    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 43313    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4500     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 44252    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4600     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 45194    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4700     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 46152    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4800     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 47113    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4900     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 48068    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5000     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 49006    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5100     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 49960    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 10.100000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "Training Time: 119.86329483985901\n",
      "EVALUATION!!!\n",
      "Mean reward: 10.0 Num episodes: 200\n",
      "---------------------------------------------\n",
      "Layer Size = 1x64\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 62.5 Num episodes: 32\n",
      "Best mean reward: -inf - Last mean reward per episode: 62.500000000000000\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 45.4 Num episodes: 44\n",
      "Best mean reward: 62.500000000000000 - Last mean reward per episode: 45.399999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 47       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 27.2     |\n",
      "| steps                   | 2697     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 62.500000000000000 - Last mean reward per episode: 133.300000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 181.699999999999989\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 141      |\n",
      "| steps                   | 16773    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 86.9 Num episodes: 23\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 86.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 128      |\n",
      "| steps                   | 29567    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 129      |\n",
      "| steps                   | 42497    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "Training Time: 118.5438551902771\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "---------------------------------------------\n",
      "Layer Size = 1x128\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 15.4 Num episodes: 130\n",
      "Best mean reward: -inf - Last mean reward per episode: 15.400000000000000\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.8 Num episodes: 205\n",
      "Best mean reward: 15.400000000000000 - Last mean reward per episode: 9.800000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 54       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 23.2     |\n",
      "| steps                   | 2298     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 15.400000000000000 - Last mean reward per episode: 111.099999999999994\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 111.099999999999994 - Last mean reward per episode: 181.699999999999989\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 144      |\n",
      "| steps                   | 16656    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 149      |\n",
      "| steps                   | 31518    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 174      |\n",
      "| steps                   | 48900    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "Training Time: 124.84450674057007\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "---------------------------------------------\n",
      "Layer Size = 1x256\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.9 Num episodes: 201\n",
      "Best mean reward: -inf - Last mean reward per episode: 9.900000000000000\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 83.3 Num episodes: 24\n",
      "Best mean reward: 9.900000000000000 - Last mean reward per episode: 83.299999999999997\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 23.2     |\n",
      "| steps                   | 2293     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 44.4 Num episodes: 45\n",
      "Best mean reward: 83.299999999999997 - Last mean reward per episode: 44.399999999999999\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 83.299999999999997 - Last mean reward per episode: 153.800000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 166.599999999999994\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 181.699999999999989\n",
      "Saving new best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 102      |\n",
      "| steps                   | 12462    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 140      |\n",
      "| steps                   | 26428    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 62.5 Num episodes: 32\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 62.500000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 133      |\n",
      "| steps                   | 39745    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "Training Time: 127.40379309654236\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "---------------------------------------------\n",
      "Layer Size = 2x2\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 50.0 Num episodes: 40\n",
      "Best mean reward: -inf - Last mean reward per episode: 50.000000000000000\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 60       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 20.2     |\n",
      "| steps                   | 1995     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 13.4     |\n",
      "| steps                   | 3331     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 13       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 11       |\n",
      "| steps                   | 4428     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 9.7      |\n",
      "| steps                   | 5398     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 6347     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 7310     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 8245     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 9188     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 10116    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 11050    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 11990    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 12934    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 13883    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 14828    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 15766    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 16705    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 17642    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.2 Num episodes: 217\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.199999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 18571    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 19522    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 20474    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 21421    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 22352    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 23293    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 24235    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 25174    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 26127    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 27073    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 28015    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 28971    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 29905    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | 9.7      |\n",
      "| steps                   | 30870    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 31819    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | 9.7      |\n",
      "| steps                   | 32785    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 33734    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 34673    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 35617    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 36574    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 37533    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 38466    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 39412    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4100     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 40368    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4200     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 41320    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4300     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 42266    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 43208    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4500     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 44142    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4600     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 45074    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4700     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 46022    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4800     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 46952    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4900     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 47900    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 48850    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.5 Num episodes: 211\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.500000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 49787    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 50.000000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "Training Time: 128.05865216255188\n",
      "EVALUATION!!!\n",
      "Mean reward: 55.5 Num episodes: 36\n",
      "---------------------------------------------\n",
      "Layer Size = 2x64\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 66.6 Num episodes: 30\n",
      "Best mean reward: -inf - Last mean reward per episode: 66.599999999999994\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 66.599999999999994 - Last mean reward per episode: 9.300000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 66.599999999999994 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 86.9 Num episodes: 23\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 86.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 60.2     |\n",
      "| steps                   | 5962     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 147      |\n",
      "| steps                   | 20634    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 17.4 Num episodes: 115\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 17.399999999999999\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 124      |\n",
      "| steps                   | 33080    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 51.3 Num episodes: 39\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 51.299999999999997\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 19.4 Num episodes: 103\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 19.399999999999999\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 80.0 Num episodes: 25\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 80.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 131      |\n",
      "| steps                   | 46166    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 14.1 Num episodes: 142\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 14.100000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 166.599999999999994\n",
      "Training Time: 137.02858328819275\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "---------------------------------------------\n",
      "Layer Size = 2x128\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.5 Num episodes: 210\n",
      "Best mean reward: -inf - Last mean reward per episode: 9.500000000000000\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 64       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 18.1     |\n",
      "| steps                   | 1795     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 11.2 Num episodes: 179\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 11.199999999999999\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 11.199999999999999 - Last mean reward per episode: 105.200000000000003\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 105.200000000000003 - Last mean reward per episode: 142.800000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 142.800000000000011 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 142.800000000000011 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 142.800000000000011 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 142.800000000000011 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 142.800000000000011 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 142.800000000000011 - Last mean reward per episode: 153.800000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 124.900000000000006\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 98.2     |\n",
      "| steps                   | 11614    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 166.599999999999994\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 60.6 Num episodes: 33\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 60.600000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 83.3 Num episodes: 24\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 83.299999999999997\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 100.000000000000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 86.6     |\n",
      "| steps                   | 20273    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 181.699999999999989\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 95.200000000000003\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 101      |\n",
      "| steps                   | 30357    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 68.9 Num episodes: 29\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 68.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 76.9 Num episodes: 26\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 76.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 52.6 Num episodes: 38\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 52.600000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 34.5 Num episodes: 58\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 34.500000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 58.8 Num episodes: 34\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 58.799999999999997\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 91.5     |\n",
      "| steps                   | 39511    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "Training Time: 145.1732199192047\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "---------------------------------------------\n",
      "Layer Size = 2x256\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 30.3 Num episodes: 66\n",
      "Best mean reward: -inf - Last mean reward per episode: 30.300000000000001\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 30.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 58       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 21.2     |\n",
      "| steps                   | 2098     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 30.300000000000001 - Last mean reward per episode: 124.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 76.9 Num episodes: 26\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 76.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 81.9     |\n",
      "| steps                   | 10290    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 124.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 29.4 Num episodes: 68\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 29.399999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 110      |\n",
      "| steps                   | 21333    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 83.3 Num episodes: 24\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 83.299999999999997\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 68.9 Num episodes: 29\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 68.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 80.0 Num episodes: 25\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 80.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 86.9 Num episodes: 23\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 86.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 116      |\n",
      "| steps                   | 32923    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 86.9 Num episodes: 23\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 86.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 18.5 Num episodes: 108\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 18.500000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 14.2 Num episodes: 141\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 14.199999999999999\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 101      |\n",
      "| steps                   | 43048    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "Training Time: 191.87267184257507\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n"
     ]
    }
   ],
   "source": [
    "# network layer tests\n",
    "for num_layers in hid_layer_list:\n",
    "    for hidden_layer_size in hid_layer_height_list:\n",
    "        layer_str = str(num_layers) + \"x\" + str(hidden_layer_size)\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"Layer Size = \" + layer_str)\n",
    "        print(\"---------------------------------------------\")\n",
    "        _layers = []\n",
    "        for l in range(num_layers):\n",
    "            _layers.append(hidden_layer_size)\n",
    "        policy_kwargs = dict(layers=_layers)\n",
    "\n",
    "        tensorboard_log = \"./DQN_Cartpole_Param/network_size_\" + layer_str\n",
    "        log_dir = \"./Results/\"\n",
    "\n",
    "        model = DQN(MlpPolicy, env, gamma=0.99, learning_rate=0.001, buffer_size=50000, policy_kwargs=policy_kwargs,\n",
    "                    exploration_fraction=0.1, exploration_final_eps=0.02, exploration_initial_eps=1.0,\n",
    "                    train_freq=1, batch_size=32, double_q=True,\n",
    "                    learning_starts=1000, target_network_update_freq=500,\n",
    "                    prioritized_replay=False, verbose=1, tensorboard_log=tensorboard_log)\n",
    "\n",
    "        best_mean_reward, n_steps = -np.inf, 0\n",
    "        evaluation_rewards = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.learn(total_timesteps=ITERS, callback=callback)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"Training Time: \" + str(elapsed_time))\n",
    "\n",
    "        model = DQN.load(log_dir + 'best_model', env=env)\n",
    "        r = evaluate(model, num_steps=EVAL_STEPS, render=False)\n",
    "\n",
    "        solved.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Learning Rate = 1e-05\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 21.3 Num episodes: 94\n",
      "Best mean reward: -inf - Last mean reward per episode: 21.300000000000001\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 10.2 Num episodes: 196\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 10.199999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 57       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 22.1     |\n",
      "| steps                   | 2183     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 32       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 12.6     |\n",
      "| steps                   | 3439     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 11       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 10.6     |\n",
      "| steps                   | 4499     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 9.8      |\n",
      "| steps                   | 5477     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 6415     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 7358     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 8294     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 9231     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 10171    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 11117    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 12061    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 13007    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 13948    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 14890    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 15824    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 9.7      |\n",
      "| steps                   | 16795    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 17739    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 18684    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 19625    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 20558    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 21496    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 22435    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 23383    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 24327    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 25271    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 26213    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 27162    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 28109    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 9.1      |\n",
      "| steps                   | 29022    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 29972    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 30922    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.5 Num episodes: 211\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.500000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 31851    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 32808    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.2 Num episodes: 217\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.199999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 33753    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 34717    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 35667    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 36606    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 37550    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 38495    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 39437    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4100     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 40385    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4200     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 41328    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4300     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 42278    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.2 Num episodes: 218\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.199999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4400     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 43232    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4500     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 44181    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4600     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 45122    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4700     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 46067    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4800     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 47002    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4900     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 47942    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5000     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 48883    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.5 Num episodes: 211\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.500000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5100     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 49817    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 21.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "Training Time: 125.50925898551941\n",
      "EVALUATION!!!\n",
      "Mean reward: 14.9 Num episodes: 134\n",
      "---------------------------------------------\n",
      "Learning Rate = 0.001\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 12.8 Num episodes: 156\n",
      "Best mean reward: -inf - Last mean reward per episode: 12.800000000000001\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 62       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 19.4     |\n",
      "| steps                   | 1921     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 29.0 Num episodes: 69\n",
      "Best mean reward: 12.800000000000001 - Last mean reward per episode: 29.000000000000000\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 29.000000000000000 - Last mean reward per episode: 142.800000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 142.800000000000011 - Last mean reward per episode: 166.599999999999994\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 181.699999999999989\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 101      |\n",
      "| steps                   | 12048    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 133      |\n",
      "| steps                   | 25372    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 76.9 Num episodes: 26\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 76.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 144      |\n",
      "| steps                   | 39731    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "Training Time: 123.7341251373291\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "---------------------------------------------\n",
      "Learning Rate = 0.1\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.2 Num episodes: 217\n",
      "Best mean reward: -inf - Last mean reward per episode: 9.199999999999999\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 20       |\n",
      "| steps                   | 1980     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.199999999999999 - Last mean reward per episode: 9.300000000000001\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 9.300000000000001 - Last mean reward per episode: 9.400000000000000\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 26       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 17.8     |\n",
      "| steps                   | 3763     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 12.2     |\n",
      "| steps                   | 4988     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 10       |\n",
      "| steps                   | 5989     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 10       |\n",
      "| steps                   | 6992     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 9.8      |\n",
      "| steps                   | 7976     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 8936     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 9897     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 10859    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.5 Num episodes: 211\n",
      "Best mean reward: 9.400000000000000 - Last mean reward per episode: 9.500000000000000\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 11797    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 12741    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 13683    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 14633    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 15562    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 9.2      |\n",
      "| steps                   | 16483    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 17418    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.2 Num episodes: 217\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.199999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 9.2      |\n",
      "| steps                   | 18340    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 19279    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 20230    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 21177    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 22137    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 23086    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 24015    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 24968    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 25919    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 26850    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 27790    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 28748    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 29696    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 30639    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3100     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 31596    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3200     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 32551    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3300     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 33494    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.2 Num episodes: 217\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.199999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3400     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 34456    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3500     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 35405    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3600     |\n",
      "| mean 100 episode reward | 9.2      |\n",
      "| steps                   | 36329    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3700     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 37276    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3800     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 38234    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3900     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 39179    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 40127    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4100     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 41068    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4200     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 42028    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4300     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 42973    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4400     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 43932    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4500     |\n",
      "| mean 100 episode reward | 9.4      |\n",
      "| steps                   | 44872    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4600     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 45806    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4700     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 46739    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 214\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4800     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 47690    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 216\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 4900     |\n",
      "| mean 100 episode reward | 9.3      |\n",
      "| steps                   | 48621    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 5000     |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 49570    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.5 Num episodes: 211\n",
      "Best mean reward: 9.500000000000000 - Last mean reward per episode: 9.500000000000000\n",
      "Training Time: 126.10655283927917\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n"
     ]
    }
   ],
   "source": [
    "# learning rate tests\n",
    "for ii in lr_list:\n",
    "    hidden_layer_size = 128\n",
    "    policy_kwargs = dict(layers=[hidden_layer_size])\n",
    "    learning_rate = ii\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"Learning Rate = \" + str(learning_rate))\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "    tensorboard_log = \"./DQN_Cartpole_Param/learning_rate_\" + str(learning_rate)\n",
    "    log_dir = \"./Results/\"\n",
    "\n",
    "    model = DQN(MlpPolicy, env, gamma=0.99, learning_rate=learning_rate, buffer_size=50000,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                exploration_fraction=0.1, exploration_final_eps=0.02, exploration_initial_eps=1.0,\n",
    "                train_freq=1, batch_size=32, double_q=True,\n",
    "                learning_starts=1000, target_network_update_freq=500,\n",
    "                prioritized_replay=False, verbose=1, tensorboard_log=tensorboard_log)\n",
    "\n",
    "    best_mean_reward, n_steps = -np.inf, 0\n",
    "    evaluation_rewards = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.learn(total_timesteps=ITERS, callback=callback)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Training Time: \" + str(elapsed_time))\n",
    "\n",
    "    model = DQN.load(log_dir + 'best_model', env=env)\n",
    "    r = evaluate(model, num_steps=EVAL_STEPS, render=False)\n",
    "    \n",
    "    solved.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Buffer Size = 32\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: -inf - Last mean reward per episode: 90.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 35.7 Num episodes: 56\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 35.700000000000003\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 22.9     |\n",
      "| steps                   | 2272     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 10.5 Num episodes: 191\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 10.500000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 29.4 Num episodes: 68\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 29.399999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 20       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 17.8     |\n",
      "| steps                   | 4047     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 55.5 Num episodes: 36\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 55.500000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 25.6 Num episodes: 78\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 25.600000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 22.8     |\n",
      "| steps                   | 6324     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 9.5      |\n",
      "| steps                   | 7271     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 21.3 Num episodes: 94\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 21.300000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 90.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 71.4 Num episodes: 28\n",
      "Best mean reward: 105.200000000000003 - Last mean reward per episode: 71.400000000000006\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 32.2     |\n",
      "| steps                   | 10491    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 105.200000000000003 - Last mean reward per episode: 111.099999999999994\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 90.9 Num episodes: 22\n",
      "Best mean reward: 111.099999999999994 - Last mean reward per episode: 90.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 111.099999999999994 - Last mean reward per episode: 133.300000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 10.7 Num episodes: 187\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 10.699999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 35.2     |\n",
      "| steps                   | 14013    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 95.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 16.5 Num episodes: 121\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 16.500000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 25.0 Num episodes: 80\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 25.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 19.2 Num episodes: 104\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 19.199999999999999\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 51.6     |\n",
      "| steps                   | 19170    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 41.6 Num episodes: 48\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 41.600000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 153.800000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 12.4 Num episodes: 161\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 12.400000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 142.800000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 64.9     |\n",
      "| steps                   | 25661    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 62.5 Num episodes: 32\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 62.500000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 153.800000000000011 - Last mean reward per episode: 181.699999999999989\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 32.8 Num episodes: 61\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 32.799999999999997\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 66.7     |\n",
      "| steps                   | 32326    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.9 Num episodes: 202\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 9.900000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 10.6     |\n",
      "| steps                   | 33383    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 9.300000000000001\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 11.2     |\n",
      "| steps                   | 34505    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 213\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 9.6      |\n",
      "| steps                   | 35467    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 48.8 Num episodes: 41\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 48.799999999999997\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 10.8 Num episodes: 185\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 10.800000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 13.8 Num episodes: 145\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 13.800000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 79.1     |\n",
      "| steps                   | 43376    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.3 Num episodes: 215\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 9.300000000000001\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 47.8     |\n",
      "| steps                   | 48156    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "Training Time: 127.3192892074585\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "---------------------------------------------\n",
      "Buffer Size = 5000\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 12.0 Num episodes: 166\n",
      "Best mean reward: -inf - Last mean reward per episode: 12.000000000000000\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 48.8 Num episodes: 41\n",
      "Best mean reward: 12.000000000000000 - Last mean reward per episode: 48.799999999999997\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 48.799999999999997 - Last mean reward per episode: 111.099999999999994\n",
      "Saving new best model\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 23       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 39.6     |\n",
      "| steps                   | 3916     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 111.099999999999994 - Last mean reward per episode: 166.599999999999994\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 166.599999999999994 - Last mean reward per episode: 199.900000000000006\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 153      |\n",
      "| steps                   | 19259    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 199.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 12.7 Num episodes: 157\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 12.699999999999999\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 54.0 Num episodes: 37\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 54.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 112      |\n",
      "| steps                   | 30460    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 181.699999999999989\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 95.200000000000003\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 121      |\n",
      "| steps                   | 42592    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 80.0 Num episodes: 25\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 80.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 199.900000000000006 - Last mean reward per episode: 105.200000000000003\n",
      "Training Time: 123.69381213188171\n",
      "EVALUATION!!!\n",
      "Mean reward: 199.9 Num episodes: 10\n",
      "---------------------------------------------\n",
      "Buffer Size = 50000\n",
      "---------------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 999\n",
      "EVALUATION!!!\n",
      "Mean reward: 11.4 Num episodes: 176\n",
      "Best mean reward: -inf - Last mean reward per episode: 11.400000000000000\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 1999\n",
      "EVALUATION!!!\n",
      "Mean reward: 9.4 Num episodes: 212\n",
      "Best mean reward: 11.400000000000000 - Last mean reward per episode: 9.400000000000000\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 24.8     |\n",
      "| steps                   | 2459     |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 2999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 11.400000000000000 - Last mean reward per episode: 95.200000000000003\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 3999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 95.200000000000003 - Last mean reward per episode: 133.300000000000011\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 4999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 5999\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n",
      "Best mean reward: 133.300000000000011 - Last mean reward per episode: 181.699999999999989\n",
      "Saving new best model\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 6999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 7999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 8999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 9999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 10999\n",
      "EVALUATION!!!\n",
      "Mean reward: 142.8 Num episodes: 14\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 142.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 11999\n",
      "EVALUATION!!!\n",
      "Mean reward: 86.9 Num episodes: 23\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 86.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 12999\n",
      "EVALUATION!!!\n",
      "Mean reward: 95.2 Num episodes: 21\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 95.200000000000003\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 107      |\n",
      "| steps                   | 13149    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 13999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 14999\n",
      "EVALUATION!!!\n",
      "Mean reward: 166.6 Num episodes: 12\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 166.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 15999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 16999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 17999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 18999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 19999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 20999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 21999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 22999\n",
      "EVALUATION!!!\n",
      "Mean reward: 153.8 Num episodes: 13\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 153.800000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 23999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 114      |\n",
      "| steps                   | 24526    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 24999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 25999\n",
      "EVALUATION!!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 26999\n",
      "EVALUATION!!!\n",
      "Mean reward: 86.9 Num episodes: 23\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 86.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 27999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 28999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 29999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 30999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 31999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 32999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 33999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 34999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 114      |\n",
      "| steps                   | 35943    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 35999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 36999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 37999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 38999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 39999\n",
      "EVALUATION!!!\n",
      "Mean reward: 100.0 Num episodes: 20\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 100.000000000000000\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 40999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 41999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 42999\n",
      "EVALUATION!!!\n",
      "Mean reward: 105.2 Num episodes: 19\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 105.200000000000003\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 43999\n",
      "EVALUATION!!!\n",
      "Mean reward: 117.6 Num episodes: 17\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 117.599999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 44999\n",
      "EVALUATION!!!\n",
      "Mean reward: 124.9 Num episodes: 16\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 124.900000000000006\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 45999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 46999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 117      |\n",
      "| steps                   | 47626    |\n",
      "--------------------------------------\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 47999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 48999\n",
      "EVALUATION!!!\n",
      "Mean reward: 111.1 Num episodes: 18\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 111.099999999999994\n",
      "-----------------------------------------------------\n",
      "Evaluating Model: 49999\n",
      "EVALUATION!!!\n",
      "Mean reward: 133.3 Num episodes: 15\n",
      "Best mean reward: 181.699999999999989 - Last mean reward per episode: 133.300000000000011\n",
      "Training Time: 124.09976410865784\n",
      "EVALUATION!!!\n",
      "Mean reward: 181.7 Num episodes: 11\n"
     ]
    }
   ],
   "source": [
    "# replay buffer tests\n",
    "for ii in buf_list:\n",
    "    hidden_layer_size = 128\n",
    "    policy_kwargs = dict(layers=[hidden_layer_size])\n",
    "    buffer_size = ii\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"Buffer Size = \" + str(buffer_size))\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "    tensorboard_log = \"./DQN_Cartpole_Param/buffer_size_\" + str(buffer_size)\n",
    "    log_dir = \"./Results/\"\n",
    "\n",
    "    model = DQN(MlpPolicy, env, gamma=0.99, learning_rate=0.001, buffer_size=buffer_size,\n",
    "                policy_kwargs=policy_kwargs,\n",
    "                exploration_fraction=0.1, exploration_final_eps=0.02, exploration_initial_eps=1.0,\n",
    "                train_freq=1, batch_size=32, double_q=True,\n",
    "                learning_starts=1000, target_network_update_freq=500,\n",
    "                prioritized_replay=False, verbose=1, tensorboard_log=tensorboard_log)\n",
    "\n",
    "    best_mean_reward, n_steps = -np.inf, 0\n",
    "    evaluation_rewards = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.learn(total_timesteps=ITERS, callback=callback)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"Training Time: \" + str(elapsed_time))\n",
    "\n",
    "    model = DQN.load(log_dir + 'best_model', env=env)\n",
    "    r = evaluate(model, num_steps=EVAL_STEPS, render=False)\n",
    "\n",
    "    solved.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward (Evaluation) for all hyperparameter combinations\n",
      "[10.0, 181.7, 199.9, 181.7, 55.5, 199.9, 199.9, 199.9, 199.9, 199.9, 14.9, 199.9, 9.3, 199.9, 199.9, 181.7]\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "\n",
    "print(\"Final reward (Evaluation) for all hyperparameter combinations\")\n",
    "print(solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
