{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to approximate the optimal controller in an MDP with unknown transition model, then one of the classical algorithms to use is Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is provided by the <b>gym</b>-environment. \n",
    "\n",
    "The task is to reach the goal state <b>G</b>, starting from initial state <b>S</b>, without falling into Hole <b>H</b> and die miserably. That, means we have to reach the goal traversing the <b>F</b>  tiles. Remember that in the Frozenworld Problem the ground is slippery, meaning if you move right, you can also move upward or downward as well (the transition model is non-deterministic).\n",
    "\n",
    "The map of the MDP is the following:\n",
    "\n",
    "        S F F F\n",
    "        F H F H\n",
    "        F F F H\n",
    "        H F F G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as the Q-learning:\n",
    "\n",
    "$Q(s,a) = Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s', a') - Q(s,a)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib as plt\n",
    "\n",
    "class QL_Agent:\n",
    "    def __init__(self, env, discount_factor, learning_rate, epsilon):\n",
    "        self.g      = discount_factor\n",
    "        self.lr     = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.terminals = [5,7,11,12,15]\n",
    "        \n",
    "        # Construct the Q table\n",
    "        action_size = env.action_space.n\n",
    "        state_size = env.observation_space.n\n",
    "        self.Qtable = np.zeros((state_size, action_size))\n",
    "        print(\"Q-Table\")\n",
    "        print(\"---------------------\")\n",
    "        print(self.Qtable)\n",
    "        # MDP.p_a: uniform action probability (25% chance an action is selected)\n",
    "        # MDP.p_s_: uniform next-state probability\n",
    "        # MDP.P_ss_(s,a): set of all possible states when taking action a in state s\n",
    "        # MDP.R(s): reward in state s\n",
    "        # MDP.A: set of all possible actions,e.g. full action space\n",
    "        # MDP.S_(s,a): next state when taking action a in state s \n",
    "        \n",
    "        # 4x4 gridworld:\n",
    "        #\"S F F F\",\n",
    "        #\"F H F H\",\n",
    "        #\"F F F H\",\n",
    "        #\"H F F G\"\n",
    "\n",
    "        #S: Start (constant starting position, reward=0)\n",
    "        #F: Ice  (introduces stochastic action, reward=0)\n",
    "        #H: Hole (ends episode, reward=0)\n",
    "        #G: Goal (ends episode, reward=1)\n",
    "        \n",
    "    def action(self,s): # epsilon-Greedy action selection\n",
    "        #LEFT = 0\n",
    "        #DOWN = 1\n",
    "        #RIGHT = 2\n",
    "        #UP = 3\n",
    "        \n",
    "        # Draw a random number between 0 and 1\n",
    "        exp_exp_tradeoff = np.random.uniform()\n",
    "        \n",
    "        # if random number greater that exploration probability, then pick the best action\n",
    "        if exp_exp_tradeoff > self.epsilon:\n",
    "            action = np.argmax(self.Qtable[s,:])\n",
    "        # else select a random action\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return(action)\n",
    "    \n",
    "    def update_QL(self,s,a,r,s_):\n",
    "       # if s_ in self.terminals:\n",
    "      #      self.Qtable[s_,:] = r\n",
    "            \n",
    "        self.Qtable[s, a] = self.Qtable[s, a] + \\\n",
    "        self.lr * (r + self.g * np.max(self.Qtable[s_, :]) - self.Qtable[s, a]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table\n",
      "---------------------\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Final Q-Table\n",
      "---------------------\n",
      "[[0.18839644 0.17838812 0.1655036  0.16261257]\n",
      " [0.11805545 0.1220741  0.10820042 0.1588671 ]\n",
      " [0.1700181  0.14765321 0.14724334 0.14658606]\n",
      " [0.08846954 0.09232435 0.05985638 0.13477897]\n",
      " [0.21563828 0.20047763 0.15178917 0.10428229]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10654764 0.11917255 0.21084548 0.046918  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.19109406 0.25979893 0.20078452 0.27958926]\n",
      " [0.32040428 0.4260055  0.28111813 0.27185174]\n",
      " [0.47302199 0.33813754 0.26591254 0.17709788]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.29605103 0.43938688 0.57862265 0.46482726]\n",
      " [0.50793589 0.64300046 0.70509125 0.61753224]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.1            # Minimum exploration probability \n",
    "decay_rate = 0.0005             # Exponential decay rate for exploration prob\n",
    "agentQL = QL_Agent(env, discount_factor=0.95,learning_rate=0.1, epsilon = max_epsilon)\n",
    "\n",
    "total_episodes = 50000\n",
    "rewards = np.zeros((total_episodes))\n",
    "for i in range(total_episodes):\n",
    "    total_rewards = 0\n",
    "    s = env.reset()  # Initializes the Frozen Lake MDP\n",
    "    while True:\n",
    "        a = agentQL.action(s)  # Apply the epsilon-Greedy policy\n",
    "        s_,r,done,_ = env.step(a)  # Observe the next state and the reward\n",
    "        \n",
    "        agentQL.update_QL(s,a,r,s_)  # Update the value function estimate using the latest transition\n",
    "                           \n",
    "        s = s_\n",
    "        \n",
    "        total_rewards += r\n",
    "        \n",
    "        # adjust exploration rate\n",
    "        agentQL.epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*i) \n",
    "        if done:  # If episode terminated (i.e., the agent fell into a hole, discovered the goal state or max. number of steps were done)\n",
    "            break \n",
    "            \n",
    "    rewards[i] = total_rewards\n",
    "\n",
    "print(\"Final Q-Table\")\n",
    "print(\"---------------------\")\n",
    "print(agentQL.Qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our Q-Learning Agent after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 0 --> Reward: 1.0\n",
      "Mean Reward: 0.0033333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 1 --> Reward: 0.0\n",
      "Mean Reward: 0.0033333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 2 --> Reward: 1.0\n",
      "Mean Reward: 0.006666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 3 --> Reward: 0.0\n",
      "Mean Reward: 0.006666666666666667\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 4 --> Reward: 0.0\n",
      "Mean Reward: 0.006666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 5 --> Reward: 1.0\n",
      "Mean Reward: 0.01\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 6 --> Reward: 1.0\n",
      "Mean Reward: 0.013333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 7 --> Reward: 0.0\n",
      "Mean Reward: 0.013333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 8 --> Reward: 1.0\n",
      "Mean Reward: 0.016666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 9 --> Reward: 1.0\n",
      "Mean Reward: 0.02\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 10 --> Reward: 0.0\n",
      "Mean Reward: 0.02\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 11 --> Reward: 1.0\n",
      "Mean Reward: 0.023333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 12 --> Reward: 1.0\n",
      "Mean Reward: 0.026666666666666665\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 13 --> Reward: 1.0\n",
      "Mean Reward: 0.03\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Evaluation Episode: 14 --> Reward: 0.0\n",
      "Mean Reward: 0.03\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 15 --> Reward: 0.0\n",
      "Mean Reward: 0.03\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 16 --> Reward: 0.0\n",
      "Mean Reward: 0.03\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 17 --> Reward: 0.0\n",
      "Mean Reward: 0.03\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 18 --> Reward: 1.0\n",
      "Mean Reward: 0.03333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 19 --> Reward: 0.0\n",
      "Mean Reward: 0.03333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 20 --> Reward: 0.0\n",
      "Mean Reward: 0.03333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 21 --> Reward: 1.0\n",
      "Mean Reward: 0.03666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 22 --> Reward: 0.0\n",
      "Mean Reward: 0.03666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 23 --> Reward: 1.0\n",
      "Mean Reward: 0.04\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 24 --> Reward: 0.0\n",
      "Mean Reward: 0.04\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 25 --> Reward: 1.0\n",
      "Mean Reward: 0.043333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 26 --> Reward: 0.0\n",
      "Mean Reward: 0.043333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 27 --> Reward: 1.0\n",
      "Mean Reward: 0.04666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 28 --> Reward: 1.0\n",
      "Mean Reward: 0.05\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 29 --> Reward: 1.0\n",
      "Mean Reward: 0.05333333333333334\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Evaluation Episode: 30 --> Reward: 0.0\n",
      "Mean Reward: 0.05333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 31 --> Reward: 1.0\n",
      "Mean Reward: 0.05666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 32 --> Reward: 1.0\n",
      "Mean Reward: 0.06\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 33 --> Reward: 0.0\n",
      "Mean Reward: 0.06\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 34 --> Reward: 1.0\n",
      "Mean Reward: 0.06333333333333332\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 35 --> Reward: 1.0\n",
      "Mean Reward: 0.06666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 36 --> Reward: 0.0\n",
      "Mean Reward: 0.06666666666666667\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 37 --> Reward: 0.0\n",
      "Mean Reward: 0.06666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 38 --> Reward: 0.0\n",
      "Mean Reward: 0.06666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 39 --> Reward: 1.0\n",
      "Mean Reward: 0.07\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 40 --> Reward: 1.0\n",
      "Mean Reward: 0.07333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 41 --> Reward: 1.0\n",
      "Mean Reward: 0.07666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 42 --> Reward: 1.0\n",
      "Mean Reward: 0.08\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 43 --> Reward: 1.0\n",
      "Mean Reward: 0.08333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 44 --> Reward: 0.0\n",
      "Mean Reward: 0.08333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 45 --> Reward: 0.0\n",
      "Mean Reward: 0.08333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 46 --> Reward: 0.0\n",
      "Mean Reward: 0.08333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 47 --> Reward: 1.0\n",
      "Mean Reward: 0.08666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 48 --> Reward: 1.0\n",
      "Mean Reward: 0.09\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Evaluation Episode: 49 --> Reward: 0.0\n",
      "Mean Reward: 0.09\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 50 --> Reward: 0.0\n",
      "Mean Reward: 0.09\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 51 --> Reward: 1.0\n",
      "Mean Reward: 0.09333333333333332\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 52 --> Reward: 1.0\n",
      "Mean Reward: 0.09666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 53 --> Reward: 1.0\n",
      "Mean Reward: 0.1\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 54 --> Reward: 1.0\n",
      "Mean Reward: 0.10333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 55 --> Reward: 0.0\n",
      "Mean Reward: 0.10333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 56 --> Reward: 1.0\n",
      "Mean Reward: 0.10666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 57 --> Reward: 1.0\n",
      "Mean Reward: 0.10999999999999999\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 58 --> Reward: 0.0\n",
      "Mean Reward: 0.10999999999999999\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 59 --> Reward: 1.0\n",
      "Mean Reward: 0.11333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 60 --> Reward: 1.0\n",
      "Mean Reward: 0.11666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 61 --> Reward: 1.0\n",
      "Mean Reward: 0.12\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 62 --> Reward: 0.0\n",
      "Mean Reward: 0.12\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 63 --> Reward: 1.0\n",
      "Mean Reward: 0.12333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 64 --> Reward: 1.0\n",
      "Mean Reward: 0.12666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 65 --> Reward: 0.0\n",
      "Mean Reward: 0.12666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 66 --> Reward: 1.0\n",
      "Mean Reward: 0.13\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Evaluation Episode: 67 --> Reward: 0.0\n",
      "Mean Reward: 0.13\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 68 --> Reward: 1.0\n",
      "Mean Reward: 0.13333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 69 --> Reward: 1.0\n",
      "Mean Reward: 0.13666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 70 --> Reward: 1.0\n",
      "Mean Reward: 0.14\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 71 --> Reward: 1.0\n",
      "Mean Reward: 0.14333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 72 --> Reward: 1.0\n",
      "Mean Reward: 0.14666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 73 --> Reward: 1.0\n",
      "Mean Reward: 0.15\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 74 --> Reward: 1.0\n",
      "Mean Reward: 0.15333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 75 --> Reward: 0.0\n",
      "Mean Reward: 0.15333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 76 --> Reward: 0.0\n",
      "Mean Reward: 0.15333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 77 --> Reward: 1.0\n",
      "Mean Reward: 0.15666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 78 --> Reward: 0.0\n",
      "Mean Reward: 0.15666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 79 --> Reward: 1.0\n",
      "Mean Reward: 0.16\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 80 --> Reward: 1.0\n",
      "Mean Reward: 0.16333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 81 --> Reward: 1.0\n",
      "Mean Reward: 0.16666666666666669\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 82 --> Reward: 0.0\n",
      "Mean Reward: 0.16666666666666669\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 83 --> Reward: 0.0\n",
      "Mean Reward: 0.16666666666666669\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 84 --> Reward: 0.0\n",
      "Mean Reward: 0.16666666666666669\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 85 --> Reward: 0.0\n",
      "Mean Reward: 0.16666666666666669\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 86 --> Reward: 1.0\n",
      "Mean Reward: 0.17\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 87 --> Reward: 0.0\n",
      "Mean Reward: 0.17\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 88 --> Reward: 1.0\n",
      "Mean Reward: 0.17333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 89 --> Reward: 1.0\n",
      "Mean Reward: 0.17666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 90 --> Reward: 1.0\n",
      "Mean Reward: 0.18\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 91 --> Reward: 0.0\n",
      "Mean Reward: 0.18\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 92 --> Reward: 1.0\n",
      "Mean Reward: 0.18333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 93 --> Reward: 0.0\n",
      "Mean Reward: 0.18333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 94 --> Reward: 1.0\n",
      "Mean Reward: 0.18666666666666668\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 95 --> Reward: 0.0\n",
      "Mean Reward: 0.18666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 96 --> Reward: 1.0\n",
      "Mean Reward: 0.19\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 97 --> Reward: 0.0\n",
      "Mean Reward: 0.19\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 98 --> Reward: 1.0\n",
      "Mean Reward: 0.19333333333333336\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 99 --> Reward: 0.0\n",
      "Mean Reward: 0.19333333333333336\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 100 --> Reward: 1.0\n",
      "Mean Reward: 0.19666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 101 --> Reward: 0.0\n",
      "Mean Reward: 0.19666666666666666\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Evaluation Episode: 102 --> Reward: 0.0\n",
      "Mean Reward: 0.19666666666666666\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 103 --> Reward: 0.0\n",
      "Mean Reward: 0.19666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 104 --> Reward: 0.0\n",
      "Mean Reward: 0.19666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 105 --> Reward: 1.0\n",
      "Mean Reward: 0.2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 106 --> Reward: 1.0\n",
      "Mean Reward: 0.20333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 107 --> Reward: 1.0\n",
      "Mean Reward: 0.20666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 108 --> Reward: 1.0\n",
      "Mean Reward: 0.21000000000000002\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 109 --> Reward: 0.0\n",
      "Mean Reward: 0.21000000000000002\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 110 --> Reward: 0.0\n",
      "Mean Reward: 0.21000000000000002\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 111 --> Reward: 0.0\n",
      "Mean Reward: 0.21000000000000002\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 112 --> Reward: 1.0\n",
      "Mean Reward: 0.21333333333333332\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 113 --> Reward: 1.0\n",
      "Mean Reward: 0.21666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 114 --> Reward: 1.0\n",
      "Mean Reward: 0.22\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 115 --> Reward: 1.0\n",
      "Mean Reward: 0.22333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 116 --> Reward: 1.0\n",
      "Mean Reward: 0.22666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 117 --> Reward: 0.0\n",
      "Mean Reward: 0.22666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 118 --> Reward: 0.0\n",
      "Mean Reward: 0.22666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 119 --> Reward: 0.0\n",
      "Mean Reward: 0.22666666666666668\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 120 --> Reward: 0.0\n",
      "Mean Reward: 0.22666666666666668\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 121 --> Reward: 1.0\n",
      "Mean Reward: 0.23\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 122 --> Reward: 1.0\n",
      "Mean Reward: 0.23333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 123 --> Reward: 0.0\n",
      "Mean Reward: 0.23333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 124 --> Reward: 0.0\n",
      "Mean Reward: 0.23333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 125 --> Reward: 1.0\n",
      "Mean Reward: 0.2366666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 126 --> Reward: 1.0\n",
      "Mean Reward: 0.24000000000000002\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 127 --> Reward: 1.0\n",
      "Mean Reward: 0.24333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 128 --> Reward: 1.0\n",
      "Mean Reward: 0.24666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 129 --> Reward: 1.0\n",
      "Mean Reward: 0.25\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 130 --> Reward: 1.0\n",
      "Mean Reward: 0.25333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 131 --> Reward: 0.0\n",
      "Mean Reward: 0.25333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 132 --> Reward: 0.0\n",
      "Mean Reward: 0.25333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "Evaluation Episode: 133 --> Reward: 0.0\n",
      "Mean Reward: 0.25333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 134 --> Reward: 1.0\n",
      "Mean Reward: 0.2566666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 135 --> Reward: 1.0\n",
      "Mean Reward: 0.26\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 136 --> Reward: 1.0\n",
      "Mean Reward: 0.26333333333333336\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 137 --> Reward: 1.0\n",
      "Mean Reward: 0.26666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 138 --> Reward: 1.0\n",
      "Mean Reward: 0.27\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 139 --> Reward: 1.0\n",
      "Mean Reward: 0.2733333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 140 --> Reward: 1.0\n",
      "Mean Reward: 0.27666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 141 --> Reward: 1.0\n",
      "Mean Reward: 0.28\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 142 --> Reward: 0.0\n",
      "Mean Reward: 0.28\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 143 --> Reward: 1.0\n",
      "Mean Reward: 0.2833333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 144 --> Reward: 0.0\n",
      "Mean Reward: 0.2833333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 145 --> Reward: 1.0\n",
      "Mean Reward: 0.2866666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 146 --> Reward: 1.0\n",
      "Mean Reward: 0.29\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 147 --> Reward: 1.0\n",
      "Mean Reward: 0.29333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 148 --> Reward: 0.0\n",
      "Mean Reward: 0.29333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 149 --> Reward: 0.0\n",
      "Mean Reward: 0.29333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 150 --> Reward: 1.0\n",
      "Mean Reward: 0.29666666666666663\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 151 --> Reward: 1.0\n",
      "Mean Reward: 0.3\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 152 --> Reward: 0.0\n",
      "Mean Reward: 0.3\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 153 --> Reward: 1.0\n",
      "Mean Reward: 0.30333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 154 --> Reward: 1.0\n",
      "Mean Reward: 0.30666666666666664\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 155 --> Reward: 0.0\n",
      "Mean Reward: 0.30666666666666664\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 156 --> Reward: 0.0\n",
      "Mean Reward: 0.30666666666666664\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 157 --> Reward: 1.0\n",
      "Mean Reward: 0.31\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 158 --> Reward: 1.0\n",
      "Mean Reward: 0.31333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 159 --> Reward: 1.0\n",
      "Mean Reward: 0.31666666666666665\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 160 --> Reward: 0.0\n",
      "Mean Reward: 0.31666666666666665\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 161 --> Reward: 1.0\n",
      "Mean Reward: 0.32\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 162 --> Reward: 0.0\n",
      "Mean Reward: 0.32\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 163 --> Reward: 1.0\n",
      "Mean Reward: 0.3233333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 164 --> Reward: 1.0\n",
      "Mean Reward: 0.32666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 165 --> Reward: 1.0\n",
      "Mean Reward: 0.33\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 166 --> Reward: 1.0\n",
      "Mean Reward: 0.3333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "Evaluation Episode: 167 --> Reward: 0.0\n",
      "Mean Reward: 0.3333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 168 --> Reward: 1.0\n",
      "Mean Reward: 0.33666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 169 --> Reward: 0.0\n",
      "Mean Reward: 0.33666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 170 --> Reward: 1.0\n",
      "Mean Reward: 0.33999999999999997\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 171 --> Reward: 1.0\n",
      "Mean Reward: 0.3433333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 172 --> Reward: 0.0\n",
      "Mean Reward: 0.3433333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 173 --> Reward: 1.0\n",
      "Mean Reward: 0.3466666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 174 --> Reward: 1.0\n",
      "Mean Reward: 0.35\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 175 --> Reward: 0.0\n",
      "Mean Reward: 0.35\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 176 --> Reward: 1.0\n",
      "Mean Reward: 0.35333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 177 --> Reward: 0.0\n",
      "Mean Reward: 0.35333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 178 --> Reward: 1.0\n",
      "Mean Reward: 0.3566666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 179 --> Reward: 1.0\n",
      "Mean Reward: 0.36\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 180 --> Reward: 0.0\n",
      "Mean Reward: 0.36\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 181 --> Reward: 1.0\n",
      "Mean Reward: 0.36333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 182 --> Reward: 1.0\n",
      "Mean Reward: 0.3666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 183 --> Reward: 1.0\n",
      "Mean Reward: 0.37\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 184 --> Reward: 1.0\n",
      "Mean Reward: 0.3733333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 185 --> Reward: 0.0\n",
      "Mean Reward: 0.3733333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 186 --> Reward: 1.0\n",
      "Mean Reward: 0.37666666666666665\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 187 --> Reward: 1.0\n",
      "Mean Reward: 0.38\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Evaluation Episode: 188 --> Reward: 0.0\n",
      "Mean Reward: 0.38\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 189 --> Reward: 1.0\n",
      "Mean Reward: 0.3833333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 190 --> Reward: 0.0\n",
      "Mean Reward: 0.3833333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 191 --> Reward: 1.0\n",
      "Mean Reward: 0.38666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 192 --> Reward: 1.0\n",
      "Mean Reward: 0.39\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 193 --> Reward: 1.0\n",
      "Mean Reward: 0.3933333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 194 --> Reward: 1.0\n",
      "Mean Reward: 0.39666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 195 --> Reward: 0.0\n",
      "Mean Reward: 0.39666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 196 --> Reward: 1.0\n",
      "Mean Reward: 0.4\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 197 --> Reward: 1.0\n",
      "Mean Reward: 0.4033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 198 --> Reward: 0.0\n",
      "Mean Reward: 0.4033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 199 --> Reward: 0.0\n",
      "Mean Reward: 0.4033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 200 --> Reward: 0.0\n",
      "Mean Reward: 0.4033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 201 --> Reward: 1.0\n",
      "Mean Reward: 0.4066666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 202 --> Reward: 1.0\n",
      "Mean Reward: 0.41000000000000003\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 203 --> Reward: 1.0\n",
      "Mean Reward: 0.41333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 204 --> Reward: 0.0\n",
      "Mean Reward: 0.41333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 205 --> Reward: 1.0\n",
      "Mean Reward: 0.41666666666666663\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 206 --> Reward: 1.0\n",
      "Mean Reward: 0.42\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 207 --> Reward: 1.0\n",
      "Mean Reward: 0.42333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 208 --> Reward: 0.0\n",
      "Mean Reward: 0.42333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 209 --> Reward: 1.0\n",
      "Mean Reward: 0.42666666666666664\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 210 --> Reward: 0.0\n",
      "Mean Reward: 0.42666666666666664\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 211 --> Reward: 1.0\n",
      "Mean Reward: 0.43\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 212 --> Reward: 1.0\n",
      "Mean Reward: 0.43333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 213 --> Reward: 1.0\n",
      "Mean Reward: 0.43666666666666665\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 214 --> Reward: 0.0\n",
      "Mean Reward: 0.43666666666666665\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 215 --> Reward: 1.0\n",
      "Mean Reward: 0.44\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 216 --> Reward: 1.0\n",
      "Mean Reward: 0.44333333333333336\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 217 --> Reward: 1.0\n",
      "Mean Reward: 0.44666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 218 --> Reward: 0.0\n",
      "Mean Reward: 0.44666666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 219 --> Reward: 1.0\n",
      "Mean Reward: 0.45\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 220 --> Reward: 1.0\n",
      "Mean Reward: 0.45333333333333337\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 221 --> Reward: 1.0\n",
      "Mean Reward: 0.45666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 222 --> Reward: 0.0\n",
      "Mean Reward: 0.45666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 223 --> Reward: 0.0\n",
      "Mean Reward: 0.45666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 224 --> Reward: 1.0\n",
      "Mean Reward: 0.45999999999999996\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 225 --> Reward: 0.0\n",
      "Mean Reward: 0.45999999999999996\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 226 --> Reward: 1.0\n",
      "Mean Reward: 0.4633333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 227 --> Reward: 0.0\n",
      "Mean Reward: 0.4633333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 228 --> Reward: 1.0\n",
      "Mean Reward: 0.4666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 229 --> Reward: 1.0\n",
      "Mean Reward: 0.47\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 230 --> Reward: 1.0\n",
      "Mean Reward: 0.47333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 231 --> Reward: 1.0\n",
      "Mean Reward: 0.4766666666666667\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "Evaluation Episode: 232 --> Reward: 0.0\n",
      "Mean Reward: 0.4766666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 233 --> Reward: 0.0\n",
      "Mean Reward: 0.4766666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 234 --> Reward: 1.0\n",
      "Mean Reward: 0.48\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 235 --> Reward: 0.0\n",
      "Mean Reward: 0.48\n",
      "  (Right)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 236 --> Reward: 0.0\n",
      "Mean Reward: 0.48\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 237 --> Reward: 0.0\n",
      "Mean Reward: 0.48\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 238 --> Reward: 0.0\n",
      "Mean Reward: 0.48\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 239 --> Reward: 0.0\n",
      "Mean Reward: 0.48\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 240 --> Reward: 1.0\n",
      "Mean Reward: 0.48333333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 241 --> Reward: 1.0\n",
      "Mean Reward: 0.4866666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 242 --> Reward: 1.0\n",
      "Mean Reward: 0.49\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 243 --> Reward: 1.0\n",
      "Mean Reward: 0.49333333333333335\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 244 --> Reward: 1.0\n",
      "Mean Reward: 0.4966666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 245 --> Reward: 1.0\n",
      "Mean Reward: 0.5\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 246 --> Reward: 1.0\n",
      "Mean Reward: 0.5033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 247 --> Reward: 0.0\n",
      "Mean Reward: 0.5033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 248 --> Reward: 0.0\n",
      "Mean Reward: 0.5033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 249 --> Reward: 1.0\n",
      "Mean Reward: 0.5066666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 250 --> Reward: 1.0\n",
      "Mean Reward: 0.51\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 251 --> Reward: 1.0\n",
      "Mean Reward: 0.5133333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 252 --> Reward: 1.0\n",
      "Mean Reward: 0.5166666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 253 --> Reward: 1.0\n",
      "Mean Reward: 0.52\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 254 --> Reward: 1.0\n",
      "Mean Reward: 0.5233333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 255 --> Reward: 1.0\n",
      "Mean Reward: 0.5266666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 256 --> Reward: 1.0\n",
      "Mean Reward: 0.53\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 257 --> Reward: 0.0\n",
      "Mean Reward: 0.53\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 258 --> Reward: 0.0\n",
      "Mean Reward: 0.53\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 259 --> Reward: 0.0\n",
      "Mean Reward: 0.53\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 260 --> Reward: 1.0\n",
      "Mean Reward: 0.5333333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 261 --> Reward: 1.0\n",
      "Mean Reward: 0.5366666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 262 --> Reward: 1.0\n",
      "Mean Reward: 0.54\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 263 --> Reward: 0.0\n",
      "Mean Reward: 0.54\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 264 --> Reward: 1.0\n",
      "Mean Reward: 0.5433333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 265 --> Reward: 1.0\n",
      "Mean Reward: 0.5466666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 266 --> Reward: 1.0\n",
      "Mean Reward: 0.55\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 267 --> Reward: 1.0\n",
      "Mean Reward: 0.5533333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 268 --> Reward: 1.0\n",
      "Mean Reward: 0.5566666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 269 --> Reward: 0.0\n",
      "Mean Reward: 0.5566666666666666\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 270 --> Reward: 1.0\n",
      "Mean Reward: 0.56\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 271 --> Reward: 1.0\n",
      "Mean Reward: 0.5633333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 272 --> Reward: 1.0\n",
      "Mean Reward: 0.5666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 273 --> Reward: 0.0\n",
      "Mean Reward: 0.5666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 274 --> Reward: 0.0\n",
      "Mean Reward: 0.5666666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 275 --> Reward: 1.0\n",
      "Mean Reward: 0.5700000000000001\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 276 --> Reward: 1.0\n",
      "Mean Reward: 0.5733333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 277 --> Reward: 0.0\n",
      "Mean Reward: 0.5733333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 278 --> Reward: 1.0\n",
      "Mean Reward: 0.5766666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 279 --> Reward: 1.0\n",
      "Mean Reward: 0.5800000000000001\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 280 --> Reward: 1.0\n",
      "Mean Reward: 0.5833333333333334\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 281 --> Reward: 1.0\n",
      "Mean Reward: 0.5866666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 282 --> Reward: 0.0\n",
      "Mean Reward: 0.5866666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 283 --> Reward: 1.0\n",
      "Mean Reward: 0.59\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 284 --> Reward: 1.0\n",
      "Mean Reward: 0.5933333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 285 --> Reward: 0.0\n",
      "Mean Reward: 0.5933333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 286 --> Reward: 1.0\n",
      "Mean Reward: 0.5966666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 287 --> Reward: 1.0\n",
      "Mean Reward: 0.6\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 288 --> Reward: 0.0\n",
      "Mean Reward: 0.6\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 289 --> Reward: 1.0\n",
      "Mean Reward: 0.6033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 290 --> Reward: 0.0\n",
      "Mean Reward: 0.6033333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 291 --> Reward: 1.0\n",
      "Mean Reward: 0.6066666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 292 --> Reward: 0.0\n",
      "Mean Reward: 0.6066666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 293 --> Reward: 1.0\n",
      "Mean Reward: 0.61\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 294 --> Reward: 1.0\n",
      "Mean Reward: 0.6133333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 295 --> Reward: 1.0\n",
      "Mean Reward: 0.6166666666666667\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 296 --> Reward: 1.0\n",
      "Mean Reward: 0.62\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Evaluation Episode: 297 --> Reward: 0.0\n",
      "Mean Reward: 0.62\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 298 --> Reward: 1.0\n",
      "Mean Reward: 0.6233333333333333\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Evaluation Episode: 299 --> Reward: 1.0\n",
      "Mean Reward: 0.6266666666666667\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "total_episodes = 300\n",
    "rewards = np.zeros((total_episodes))\n",
    "agentQL.epsilon = 0.0 # Greedy Policy\n",
    "\n",
    "for i in range(total_episodes):\n",
    "    total_rewards = 0\n",
    "    s = env.reset()  # Initializes the Frozen Lake MDP\n",
    "    while True:\n",
    "        a = agentQL.action(s)  # Apply the epsilon-Greedy policy\n",
    "        s_,r,done,_ = env.step(a)  # Observe the next state and the reward\n",
    "        \n",
    "#         agentQL.update_QL(s,a,r,s_)  # Update the value function estimate using the latest transition\n",
    "                           \n",
    "        s = s_\n",
    "        \n",
    "        total_rewards += r\n",
    "        \n",
    "        if done:  # If episode terminated (i.e., the agent fell into a hole, discovered the goal state or max. number of steps were done)\n",
    "             # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            break \n",
    "            \n",
    "    rewards[i] = total_rewards\n",
    "    print(\"Evaluation Episode: \" + str(i) + \" --> Reward: \" + str(rewards[i]))\n",
    "    print(\"Mean Reward: \" + str(np.sum(rewards / total_episodes)))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
