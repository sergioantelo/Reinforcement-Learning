{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the MDP is known, then we can use Value Iteration to calculate the actual values of V and use them to infer the optimal policy. If we don't know the MDP, we can use MC or TD learning. For this exercise you will implement the TD learning algorithm in the same MDP as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is provided by the <b>gym</b>-environment. \n",
    "\n",
    "The task is to reach the goal state <b>G</b>, starting from initial state <b>S</b>, without falling into Hole <b>H</b> and die miserably. That, means we have to reach the goal traversing the <b>F</b>  tiles. Remember that in the Frozenworld Problem the ground is slippery, meaning if you move right, you can also move upward or downward as well (the transition model is non-deterministic).\n",
    "\n",
    "The map of the MDP is the following:\n",
    "\n",
    "        S F F F\n",
    "        F H F H\n",
    "        F F F H\n",
    "        H F F G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TD learning we do not have access to the underlying MDP, thus we cannot estimate the <b>optimal</b> value function V. What we can do instead is estimate the value function of the policy $\\pi$ applied to the MDP using the TD(0) method:\n",
    "\n",
    "$V^{\\pi} \\left( s \\right) = V^{\\pi} \\left( s \\right) + \\alpha \\left[ r + \\gamma V^{\\pi} \\left( s' \\right) - V^{\\pi} \\left( s \\right) \\right]$\n",
    "\n",
    "Note here, that unlike Monte Carlo prediction, this update is performed after each transition in the episode, meaning we are in state $s$, perform action $a$, receive reward $r$ and end up in a new state $s'$.\n",
    "\n",
    "<b>ATTENTION</b>: As before, an importan aspect to consider is that special care needs to be taken when we are at a terminal state (e.g., in the Goal <b>G</b> or Hole <b>H</b> states of the Frozen Lake MDP). Here, no matter what action we execute, we cannot \"escape\" this state, so the Value Iteration implementation should be adapted accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TD(0) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from FrozenLake import FrozenLakeEnv\n",
    "\n",
    "class TD_Agent:\n",
    "    def __init__(self, discount_factor, learning_rate):\n",
    "        self.g      = discount_factor\n",
    "        self.lr     = learning_rate\n",
    "        \n",
    "        self.MDP = FrozenLakeEnv()\n",
    "        self.MDP.seed(0)\n",
    "        self.terminals = [5,7,11,12,15]\n",
    "\n",
    "        \n",
    "        # MDP.p_a: uniform action probability (25% chance an action is selected)\n",
    "        # MDP.p_s_: uniform next-state probability\n",
    "        # MDP.P_ss_(s,a): set of all possible states when taking action a in state s\n",
    "        # MDP.R(s): reward in state s\n",
    "        # MDP.A: set of all possible actions,e.g. full action space\n",
    "        # MDP.S_(s,a): next state when taking action a in state s \n",
    "        \n",
    "        # 4x4 gridworld:\n",
    "        #\"S F F F\",\n",
    "        #\"F H F H\",\n",
    "        #\"F F F H\",\n",
    "        #\"H F F G\"\n",
    "\n",
    "        #S: Start (constant starting position, reward=0)\n",
    "        #F: Ice  (introduces stochastic action, reward=0)\n",
    "        #H: Hole (ends episode, reward=0)\n",
    "        #G: Goal (ends episode, reward=1)\n",
    "        \n",
    "        # states = {0,1,...,15}\n",
    "        self.V = np.zeros(shape=(16), dtype=float)   \n",
    "        \n",
    "    def action(self,s):\n",
    "        #LEFT = 0\n",
    "        #DOWN = 1\n",
    "        #RIGHT = 2\n",
    "        #UP = 3\n",
    "            \n",
    "        # use uniform and random policy\n",
    "        action = np.random.uniform()\n",
    "\n",
    "        if action <= 0.25: return 0\n",
    "        if action <= 0.50: return 1\n",
    "        if action <= 0.75: return 2\n",
    "        if action <= 1.00: return 3\n",
    "    \n",
    "    # TD Learning    \n",
    "    def update_TD(self,s,a,r,s_):\n",
    "        self.V[s]  = self.V[s] + self.lr*(r+self.g*self.V[s_]-self.V[s])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentTD = TD_Agent(discount_factor=0.95,learning_rate=0.005)\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    s = env.reset()  # Initializes the Frozen Lake MDP\n",
    "    while True:\n",
    "        a = agentTD.action(s)  # Apply the random policy\n",
    "        s_,r,done,_ = env.step(a)  # Observe the next state and the reward\n",
    "        \n",
    "        agentTD.update_TD(s,a,r,s_)  # Update the value function estimate using the latest transition\n",
    "                           \n",
    "        s = s_\n",
    "        \n",
    "        if done:  # If episode terminated (i.e., the agent fell into a hole, discovered the goal state or max. number of steps were done)\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00766771 0.00741708 0.01459295 0.00653726]\n",
      " [0.00957835 0.         0.03139907 0.        ]\n",
      " [0.02289711 0.06945704 0.12939718 0.        ]\n",
      " [0.         0.13918085 0.4354858  0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "V = agentTD.V.copy()\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(V.reshape(4,4), cmap='gray', interpolation='none', clim=(0,1))\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(4)-.5)\n",
    "ax.set_yticks(np.arange(4)-.5)\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "Y, X = np.mgrid[0:4, 0:4]\n",
    "a2uv = {0: (-1, 0), 1:(0, -1), 2:(1,0), 3:(-1, 0)}\n",
    "for y in range(4):\n",
    "    for x in range(4):\n",
    "        u, v = a2uv[a]\n",
    "        plt.text(x, y, str(agentTD.MDP.unwrapped.desc[y,x].item().decode()),\n",
    "                 color='g', size=12,  verticalalignment='center',\n",
    "                 horizontalalignment='center', fontweight='bold')\n",
    "plt.grid(color='b', lw=2, ls='-')\n",
    "\n",
    "print(V.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find a better policy based on the estimated Value function? \n",
    "\n",
    "Remember that you do not know the transition model of the MDP (i.e., if you are in state $s$ = 1 and take action $a$ = right, you don't know the possible next states $s'$)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
